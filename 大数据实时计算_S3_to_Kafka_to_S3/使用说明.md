### 环境
- scala 2.11.8
- flink

### 用户配置
在S3reader.scala文件中修改以下变量：  
- accessKey = ' '       #输入自己的密钥  
- secretKey = ' '       #输入自己的密钥  
- endpoint = ' '     #输入s3服务器的地址  
- bucket = ' '  #s3桶的名称
- key = ' '    #读取进入kafka的key的名称
- topic = ' '   #kafka的主题名称
- bootstrapServers = ' '   #kafka的服务器地址

在Main.scala文件中修改以下变量：  
- accessKey = ' '       #输入自己的密钥  
- secretKey = ' '       #输入自己的密钥  
- endpoint = ' '     #输入s3服务器的地址  
- bucket = ' '  #s3桶的名称
- keyPrefix = ' '    #读取进入kafka的key的名称
- inputTopic = ' '   #kafka的主题名称
- bootstrapServers = ' '   #kafka的服务器地址
- filename = ' '   #从kafka下载到本地的文件名

### 运行
直接运行Main.scala文件即可，在此之前要先给Model添加flink依赖和scala依赖。  

### 结果
根据origin字段排序后的结果被写入S3桶里面：
