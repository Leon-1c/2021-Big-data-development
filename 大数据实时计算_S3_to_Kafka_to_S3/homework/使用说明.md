### 环境
- scala 2.11.8
- flink

### 用户配置
在S3reader.scala文件中修改以下变量：  
- accessKey = ' '       #输入自己的密钥  
- secretKey = ' '       #输入自己的密钥  
- endpoint = ' '     #输入s3服务器的地址  
- bucket = ' '  #s3桶的名称
- key = ' '    #读取进入kafka的key的名称
- topic = ' '   #kafka的主题名称
- bootstrapServers = ' '   #kafka的服务器地址

在Main.scala文件中修改以下变量：  
- accessKey = ' '       #输入自己的密钥  
- secretKey = ' '       #输入自己的密钥  
- endpoint = ' '     #输入s3服务器的地址  
- bucket = ' '  #s3桶的名称
- keyPrefix = ' '    #读取进入kafka的key的名称
- inputTopic = ' '   #kafka的主题名称
- bootstrapServers = ' '   #kafka的服务器地址
- filename = ' '   #从kafka下载到本地的文件名

### 运行
直接运行Main.scala文件即可，在此之前要先给Model添加flink依赖和scala依赖。  

### 结果
根据origin字段归类后的结果被写入S3桶里面：
![showfield](https://github.com/Leon-1c/2021-Big-data-development/blob/3c1dcafd7a01367280be20c5013cb3bce3a65922/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97_S3_to_Kafka_to_S3/homework/result.jpg)
